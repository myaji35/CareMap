import { NextRequest, NextResponse } from 'next/server';
import { crawlInstitutions } from '@/lib/crawler';
import { prisma } from '@/lib/prisma';
import fs from 'fs';
import path from 'path';

const TEMP_DIR = path.join(process.cwd(), '.crawler-temp');
const DATA_FILE = path.join(TEMP_DIR, 'crawled-data.json');
const STATUS_FILE = path.join(TEMP_DIR, 'status.json');

// ì„ì‹œ ë””ë ‰í† ë¦¬ ìƒì„±
if (!fs.existsSync(TEMP_DIR)) {
  fs.mkdirSync(TEMP_DIR, { recursive: true });
}

export async function POST(request: NextRequest) {
  try {
    const body = await request.json();
    const { maxPages = 1, crawlerId } = body;

    // ìƒíƒœ í™•ì¸
    let status: any = { isCrawling: false };
    if (fs.existsSync(STATUS_FILE)) {
      const statusContent = fs.readFileSync(STATUS_FILE, 'utf-8');
      status = JSON.parse(statusContent);

      // í¬ë¡¤ë§ ì¤‘ì´ì§€ë§Œ 5ë¶„ ì´ìƒ ì§„í–‰ ìƒíƒœ ì—…ë°ì´íŠ¸ê°€ ì—†ìœ¼ë©´ ì´ˆê¸°í™”
      if (status.isCrawling && status.lastUpdate) {
        const lastUpdateTime = new Date(status.lastUpdate).getTime();
        const now = Date.now();
        const fiveMinutes = 5 * 60 * 1000;

        if (now - lastUpdateTime > fiveMinutes) {
          console.log('âš  í¬ë¡¤ë§ ìƒíƒœê°€ 5ë¶„ ì´ìƒ ì—…ë°ì´íŠ¸ë˜ì§€ ì•ŠìŒ - ìƒíƒœ ì´ˆê¸°í™”');
          status.isCrawling = false;
        }
      }
    }

    // DBì—ì„œ ì‹¤í–‰ ì¤‘ì¸ ì‘ì—… í™•ì¸
    const runningJob = await prisma.crawlerJob.findFirst({
      where: { status: 'running' },
      orderBy: { startedAt: 'desc' },
    });

    if (runningJob || status.isCrawling) {
      return NextResponse.json(
        { error: 'í¬ë¡¤ë§ì´ ì´ë¯¸ ì‹¤í–‰ ì¤‘ì…ë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•˜ê±°ë‚˜ "ì¤‘ì§€" ë²„íŠ¼ì„ í´ë¦­í•˜ì„¸ìš”.' },
        { status: 409 }
      );
    }

    // DBì— ìƒˆ ì‘ì—… ê¸°ë¡ ìƒì„±
    const job = await prisma.crawlerJob.create({
      data: {
        crawlerId: crawlerId || null,
        status: 'running',
        totalPages: maxPages,
        crawledCount: 0,
      },
    });

    console.log(`\nğŸš€ í¬ë¡¤ë§ ì‘ì—… ì‹œì‘ (Job ID: ${job.id})`);
    console.log(`   - ì´ í˜ì´ì§€: ${maxPages}`);
    console.log(`   - ì‹œì‘ ì‹œê°„: ${job.startedAt.toISOString()}\n`);

    // í¬ë¡¤ë§ ìƒíƒœ ì—…ë°ì´íŠ¸
    fs.writeFileSync(STATUS_FILE, JSON.stringify({
      isCrawling: true,
      jobId: job.id,
      lastUpdate: new Date().toISOString()
    }));
    fs.writeFileSync(DATA_FILE, JSON.stringify({ institutions: [] }));

    // ë°±ê·¸ë¼ìš´ë“œì—ì„œ í¬ë¡¤ë§ ì‹¤í–‰
    crawlInstitutions(maxPages, async (progress) => {
      // ì§„í–‰ ìƒíƒœë¥¼ status.jsonê³¼ DBì— ì €ì¥
      const statusData = {
        isCrawling: true,
        jobId: job.id,
        lastUpdate: new Date().toISOString(),
        progress: {
          current: progress.current,
          total: progress.total,
          percentage: progress.percentage,
          message: progress.message,
        },
      };
      fs.writeFileSync(STATUS_FILE, JSON.stringify(statusData));

      // DB ì—…ë°ì´íŠ¸
      await prisma.crawlerJob.update({
        where: { id: job.id },
        data: { crawledCount: progress.current },
      });

      console.log(
        `[í¬ë¡¤ë§ ì§„í–‰] ${progress.percentage}% - ${progress.message}`
      );
    })
      .then(async (institutions) => {
        fs.writeFileSync(
          DATA_FILE,
          JSON.stringify({ institutions, timestamp: new Date().toISOString(), jobId: job.id })
        );
        fs.writeFileSync(
          STATUS_FILE,
          JSON.stringify({
            isCrawling: false,
            jobId: job.id,
            progress: {
              current: maxPages,
              total: maxPages,
              percentage: 100,
              message: `í¬ë¡¤ë§ ì™„ë£Œ: ${institutions.length}ê°œ ê¸°ê´€ ìˆ˜ì§‘`,
            },
          })
        );

        // DB ì‘ì—… ì™„ë£Œ ì²˜ë¦¬
        await prisma.crawlerJob.update({
          where: { id: job.id },
          data: {
            status: 'completed',
            completedAt: new Date(),
            crawledCount: institutions.length,
          },
        });

        console.log(`âœ… í¬ë¡¤ë§ ì™„ë£Œ (Job ID: ${job.id})`);
        console.log(`   - ìˆ˜ì§‘ ê¸°ê´€ ìˆ˜: ${institutions.length}`);
        console.log(`   - ì™„ë£Œ ì‹œê°„: ${new Date().toISOString()}\n`);
      })
      .catch(async (error) => {
        console.error('âŒ í¬ë¡¤ë§ ì‹¤íŒ¨:', error);

        // DB ì‘ì—… ì‹¤íŒ¨ ì²˜ë¦¬
        await prisma.crawlerJob.update({
          where: { id: job.id },
          data: {
            status: 'failed',
            completedAt: new Date(),
            errorMessage: error instanceof Error ? error.message : 'ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜',
          },
        });

        fs.writeFileSync(
          STATUS_FILE,
          JSON.stringify({
            isCrawling: false,
            jobId: job.id,
            progress: null,
            error: error instanceof Error ? error.message : 'ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜',
          })
        );
        fs.writeFileSync(DATA_FILE, JSON.stringify({ institutions: [] }));
      });

    return NextResponse.json({
      status: 'started',
      message: 'í¬ë¡¤ë§ì´ ì‹œì‘ë˜ì—ˆìŠµë‹ˆë‹¤',
      maxPages,
      jobId: job.id,
    });
  } catch (error) {
    console.error('í¬ë¡¤ëŸ¬ ì‹œì‘ ì˜¤ë¥˜:', error);
    fs.writeFileSync(STATUS_FILE, JSON.stringify({ isCrawling: false }));
    return NextResponse.json(
      { error: 'í¬ë¡¤ëŸ¬ ì‹œì‘ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤' },
      { status: 500 }
    );
  }
}
